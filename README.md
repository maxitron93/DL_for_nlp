# Overview
## 1 Introduction to NLP
Natural language processing enables humans and machines to communicate in natural human language
There are three broad applications of natural language processing:
    
1. Speech recognition
2. Natural language understanding
3. Natural language generation

Language is a complicated thing, and so text is required to go through several phases before it can make sense to a machine.

Since machine learning and deep learning models work best with numerical data, it is necessary to transform preprocessed corpora into numerical form. 

This is where word embeddings come into the picture; they are real-value vector representations of words that aid models in predicting and understanding words. The two main algorithms used to generate word embeddings are Word2Vec and GloVe.

## 2 Applications of NLP
Two ways of better understanding language that allow machines to contribute to the real world are POS tagging and named entity recognition:

- POS tagging is the process of assigning POS tags to individual words so that the machine can learn context.
- Named entity recognition is recognizing and categorizing named entities to extract valuable information from corpora.

There are distinctions in the way these processes are performed: the algorithms can be supervised or unsupervised, and the approach can be rule-based or stochastic. Either way, the goal is the same, that is, to comprehend and communicate with humans in their natural language.

## 3 Introduction to Neural Networks
Artificial neural networks are frameworks that are incorporated by deep learning models and have proven to be increasingly efficient and accurate. 

Neural networks train and correct themselves, with the help of the loss function, the gradient descent algorithm and backpropagation.

## 4 Convolutional Neural Networks (CNNs)

## 5 Recurrent Neural Netowkrs (RNNs)

## 6 Gated Recurrent Units (GRUs)
A GRU is an extension of a simple RNN, which helps to combat the vanishing gradient problem by allowing the model to learn long-term dependencies in the text structure. 

GRUs can be applied to a wide variety of problems including sentiment classification and text generation.

## 7 Long Short-Term Memory (LSTMs)
LSTM units are another possible remedy to the vanishing gradient problem.

## 8 SotA for NLP


## 9 NLP Project Workflow

